# GPT Implementation from Scratch ğŸš€  

A minimal yet educational implementation of **GPT (Generative Pretrained Transformer)** from scratch using **PyTorch**.  

This repo walks through how GPT works step by step â€” from **self-attention** to **multi-head attention**, **transformer blocks**, and finally, **text generation** at both **character-level** and **word-level**.  

---

## ğŸ“‚ Project Structure  

```
GPT-implementation_from_scratch/
â”‚â”€â”€ Explaining/               # Images & text explanations of GPT components
â”‚â”€â”€ GPT/                      # Core GPT implementation
â”‚   â”œâ”€â”€ single_head.py
â”‚   â”œâ”€â”€ multi_head.py
â”‚   â”œâ”€â”€ transformer_block.py
â”‚   â””â”€â”€ Gpt_class.py
â”‚â”€â”€ Character_level/          # Character-level GPT (training attempted, no results saved)
â”‚   â”œâ”€â”€ solution.py           # Generate function (character-level)
â”‚   â””â”€â”€ testing.py
â”‚â”€â”€ Word_level/               # Word-level GPT (trained model + outputs)
â”‚   â”œâ”€â”€ solution.py           # Generate function (word-level)
â”‚   â”œâ”€â”€ testing.py            # Training + generation script
â”‚   â””â”€â”€ outputs/              # Screenshots of sample outputs
â”‚â”€â”€ Doc_reader/               # Utilities to read input data
â”‚   â”œâ”€â”€ doc_up.py             # Read from .docx
â”‚   â””â”€â”€ pdf_up.py             # Read from .pdf
â”‚â”€â”€ Conversation.docx         # Training data (sample conversation text)
â”‚â”€â”€ dcmachinebook.pdf         # Training data(DC Machines book, ~17k chars until ch1)
â”‚â”€â”€ requirements.txt          # Dependencies of this project
â”‚â”€â”€ wrd_trained_model.pkl     # Pretrained Model (for dat until ch1)
â”‚â”€â”€ README.md                 # Project documentation
```

---

## âš™ï¸ Requirements  

Install dependencies:  

```bash
pip install torch torchtyping python-docx PyMuPDF
OR
pip install -r requirements.txt
```

ğŸ“Œ Notes:  
- `torch` â†’ Core PyTorch library  
- `torchtyping` â†’ For type annotations with tensors  
- `python-docx` â†’ Read `.docx` files  
- `PyMuPDF` (`pip install PyMuPDF`) â†’ Read `.pdf` files (via `import fitz`)  
- Standard libraries (`os`, etc.) are already included in Python.  


---

## ğŸ‹ï¸ Training  

To train the **word-level GPT** from scratch:  

```bash
python -m Word_level.testing
```

This will:  
1. Read training data (`Conversation.docx` or `dcmachinebook.pdf`)  
2. Build vocabulary  
3. Train a GPT model  
4. Save the trained model as **`wrd_trained_model.pkl`**  

ğŸ“Œ Early Stopping is included â†’ if loss does not improve for 5 epochs (`min_delta=0.001`), training stops and the **best model** is saved.  

---

## âœ¨ Text Generation  

Text is generated using the `Solution.generate()` function with parameters:  

- `context` â†’ The seed text (e.g., `"dc machine is"`)  
- `new_chars` â†’ How many characters/wprds to generate
- `context_length` â†’ How many tokens the model can look back  
- `temperature` â†’ Controls creativity (higher = more random)  
- `top_k` â†’ Sample only from top-k most likely words  

Example (word-level):  

```python
seed = "dc machine is" # this will be the starting point of model
encoded_seed = [stoi[w] for w in seed.split() if w in stoi]
start_context = torch.tensor([encoded_seed], dtype=torch.long).to(device)

generated_answer = solution.generate(
    model=model,
    new_chars=100,
    context=start_context,
    context_length=block_size,
    int_to_char=itos,
    temperature=0.8,
    top_k=20,
    join_with=" "
)

print("Q:", seed)
print("A:", generated_answer)
```

---

## ğŸ“Š Results  

- **Character-level GPT** â†’ training attempted, but no usable results saved.  
- **Word-level GPT** â†’ trained on **~17,000 characters (~3,500 words) until Chapter 1 of DC Machines book**.  
- Trained model saved as **`wrd_trained_model.pkl`** and can be reused for inference.  

Screenshots of sample outputs are available in `Word_level/outputs/`.  

---

## ğŸ”— Clone the Repository  

```bash
git clone https://github.com/moabs-dev/GPT-implementation_from_scratch
```

---

## ğŸ“œ Notes  

- This repo is **educational only** â€” showing how GPT works step by step.  
- No frontend/backend provided.  
- You can experiment with your own `.docx` or `.pdf` datasets by placing them inside the repo and updating the training scripts.  

---

âœ¨ Enjoy exploring how GPT works under the hood! 
Explanation of how attention works in Gpt class is explained in folder `explaining`
